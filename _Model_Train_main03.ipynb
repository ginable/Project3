{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef1Tmt02mbh9"
   },
   "source": [
    "### Model_Train_00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "743cxA_MmjAZ"
   },
   "source": [
    "### 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "_H3my2JjHi2Z",
    "outputId": "f1d90616-1e2d-409e-c97c-c42bc6a82eaf"
   },
   "outputs": [],
   "source": [
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9M3mRboVHmLY",
    "outputId": "56af6806-7a36-4be2-9786-28d4b11475a8"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcdrLjs7mtxm"
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv4f8IbmHP0Q"
   },
   "source": [
    "### 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Mm5w8RqTHPgG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import cv2\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUFS9iOwCbVv",
    "outputId": "c09c75c5-0788-4620-dc8a-127c9ab24146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "collapsed": true,
    "id": "3S-hZ3AoOJ8L",
    "outputId": "811a84ce-275c-4044-dce0-f32fc886dca5"
   },
   "outputs": [],
   "source": [
    "# 비디오 zip 파일 폴더 및 저장 폴더 경로\n",
    "video_folder = '/content/drive/MyDrive/datasets/Video_maindata'  # 비디오가 저장될 폴더\n",
    "output_folder = '/content/drive/MyDrive/datasets/Image_maindata'  # 얼굴 이미지 저장 폴더\n",
    "\n",
    "# 압축 파일 리스트\n",
    "zip_file_paths = [\n",
    "    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_00.zip',\n",
    "    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_01.zip',\n",
    "    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_02.zip',\n",
    "    '/content/drive/MyDrive/datasets/train_sample_videos.zip'\n",
    "]\n",
    "\n",
    "# 저장 폴더가 없으면 생성\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "# 각 ZIP 파일 압축 해제\n",
    "for zip_file_path in zip_file_paths:\n",
    "    print(f\"Extracting {zip_file_path}...\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(video_folder)\n",
    "    print(f\"Finished extracting {zip_file_path}\")\n",
    "\n",
    "print(\"모든 ZIP 파일이 성공적으로 압축 해제되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aqGyD_WJLJ_",
    "outputId": "9eaad198-fc0c-4ac5-ac13-07e33dee8fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 4: GPU 또는 CPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANErx1QmJNlu",
    "outputId": "49e4789a-f9b7-4e90-e27d-8584cf2b3698"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: GPU를 사용한 MTCNN 초기화\n",
    "mtcnn = MTCNN(keep_all=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DX2OXTX-JQzs"
   },
   "outputs": [],
   "source": [
    "# Step 6: 추가 설정\n",
    "frame_interval = 10  # 프레임 간격\n",
    "resize_dim = (224, 224)  # 얼굴 이미지 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "daf_tqRGJGs4"
   },
   "outputs": [],
   "source": [
    "# Step 7: 비디오 처리 함수\n",
    "def process_video(video_path, output_folder, mtcnn, frame_interval, resize_dim):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    extracted_count = 0\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    # 비디오별 하위 폴더 생성\n",
    "    video_output_folder = os.path.join(output_folder, video_name)\n",
    "    os.makedirs(video_output_folder, exist_ok=True)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 프레임 간격에 따라 샘플링\n",
    "        if frame_count % frame_interval == 0:\n",
    "            # OpenCV BGR 이미지를 PIL RGB로 변환\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "            # MTCNN을 통한 얼굴 감지\n",
    "            boxes, _ = mtcnn.detect(pil_img)\n",
    "            if boxes is not None:\n",
    "                for i, box in enumerate(boxes):\n",
    "                    # 얼굴 크롭 및 리사이즈\n",
    "                    face = pil_img.crop((box[0], box[1], box[2], box[3])).resize(resize_dim)\n",
    "                    face.save(os.path.join(video_output_folder, f\"{video_name}_face_{extracted_count}.jpg\"))\n",
    "                    extracted_count += 1\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LMoVwCnxuSW",
    "outputId": "93a88efb-99d1-4292-f111-bd84695b7bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 15128개의 비디오 파일이 발견되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: 하위 폴더의 모든 비디오 파일을 가져오기\n",
    "video_files = []\n",
    "for root, dirs, files in os.walk(video_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.mp4'):\n",
    "            video_files.append(os.path.join(root, file))\n",
    "\n",
    "if not video_files:\n",
    "    print(\"No .mp4 files found in the extracted folders. Check if the ZIP files contain .mp4 videos.\")\n",
    "else:\n",
    "    print(f\"총 {len(video_files)}개의 비디오 파일이 발견되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "collapsed": true,
    "id": "6Ckkyp5vJVMA",
    "outputId": "57ee1f58-1e80-4b43-997f-f5887203e927"
   },
   "outputs": [],
   "source": [
    "# Step 9: 모든 비디오 파일 처리\n",
    "for folder in os.listdir(video_folder):\n",
    "    folder_path = os.path.join(video_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n",
    "        for video_file in tqdm(video_files, desc=f\"{folder} 비디오 처리 중\"):\n",
    "            video_path = os.path.join(folder_path, video_file)\n",
    "            process_video(video_path, output_folder, mtcnn, frame_interval, resize_dim)\n",
    "            print(f\"{video_file} 처리 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLfcm_Axmxwm"
   },
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adE7UN1cm0dI"
   },
   "source": [
    "### 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kHQIed3icyT9",
    "outputId": "c14e35f0-9722-4f64-d917-4bd4ba5b822b"
   },
   "outputs": [],
   "source": [
    "# EfficientNet-PyTorch 설치\n",
    "!pip install efficientnet-pytorch\n",
    "\n",
    "# pretrainedmodels 설치 (Xception 모델용)\n",
    "!pip install pretrainedmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9RfXTuc7ch5w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from pretrainedmodels import xception\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dKE_SEQ-cqoK"
   },
   "outputs": [],
   "source": [
    "# SSL 인증서 검증 비활성화\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# 커스텀 데이터셋 정의\n",
    "class FaceForensicsDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 하위 디렉터리까지 포함하여 이미지와 레이블을 로드\n",
    "        for root, _, files in os.walk(image_folder):\n",
    "            for img in files:\n",
    "                if img.endswith(('.jpg', '.png')):\n",
    "                    img_path = os.path.join(root, img)\n",
    "                    self.images.append(img_path)\n",
    "                    # 파일명에 \"REAL\" 포함 여부로 라벨을 설정\n",
    "                    self.labels.append(1 if \"REAL\" in img else 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # NaN 검사\n",
    "        if torch.isnan(image).any() or torch.isinf(image).any():\n",
    "            print(f\"Data issue in image {img_path}\")\n",
    "        return image, label\n",
    "\n",
    "# 데이터 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 커스텀 데이터셋 인스턴스 생성\n",
    "image_folder = '/content/drive/MyDrive/datasets/Image_maindata'\n",
    "dataset = FaceForensicsDataset(image_folder=image_folder, transform=transform)\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlVvE_Mpccqy",
    "outputId": "8e7d56d9-973a-4178-ea1c-d21fc4333151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# GPU 장치 설정 (Colab에서는 'cuda' 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델 설정\n",
    "xception_model = xception(num_classes=1000, pretrained='imagenet')\n",
    "xception_model.last_linear = nn.Linear(xception_model.last_linear.in_features, 2)\n",
    "xception_model = xception_model.to(device)\n",
    "\n",
    "efficientnet_model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2)\n",
    "efficientnet_model = efficientnet_model.to(device)\n",
    "\n",
    "resnet_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 2)\n",
    "resnet_model = resnet_model.to(device)  # 모델을 MPS에 올리기\n",
    "# 손실 함수 및 최적화 함수 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(\n",
    "    list(xception_model.parameters()) +\n",
    "    list(efficientnet_model.parameters()) +\n",
    "    list(resnet_model.parameters()),\n",
    "    lr=1e-5  # 학습률을 낮추어 설정\n",
    ")\n",
    "\n",
    "# NaN 검사 함수\n",
    "def check_nan(tensor, name=\"\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predict(models, images):\n",
    "    outputs = [model(images) for model in models]\n",
    "    outputs = torch.stack(outputs).mean(dim=0)\n",
    "    return outputs\n",
    "\n",
    "# 학습 및 검증 함수 정의\n",
    "def train_ensemble_model(models, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        for model in models:\n",
    "            model.train()\n",
    "\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ensemble_predict(models, images)\n",
    "\n",
    "            # NaN 검사\n",
    "            if check_nan(outputs, \"outputs\"):\n",
    "                return\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            if check_nan(loss, \"loss\"):\n",
    "                return\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "\n",
    "        train_acc = train_correct / len(train_dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = ensemble_predict(models, images)\n",
    "                if check_nan(outputs, \"validation outputs\"):\n",
    "                    return\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                if check_nan(loss, \"validation loss\"):\n",
    "                    return\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(val_dataset)\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpYi6mUvdDSt",
    "outputId": "d0c067b2-3a4a-4272-e932-11f78cda0834"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5:   0%|          | 16/9085 [03:23<32:38:24, 12.96s/it]"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "models = [xception_model, efficientnet_model, resnet_model]\n",
    "train_ensemble_model(models, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3vHOi0wZ22c"
   },
   "outputs": [],
   "source": [
    "# 모델 저장 경로 설정\n",
    "output_dir = '/content/drive/MyDrive/datasets'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(xception_model.state_dict(), os.path.join(output_dir, 'xception_model_02.pth'))\n",
    "torch.save(efficientnet_model.state_dict(), os.path.join(output_dir, 'efficientnet_model_02.pth'))\n",
    "torch.save(resnet_model.state_dict(), os.path.join(output_dir, 'resnet_model_02.pth'))\n",
    "\n",
    "print(\"모델이 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSzv5FBnw11K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
