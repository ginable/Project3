# DeepFake Starter Kit

https://www.kaggle.com/code/gpreda/deepfake-starter-kit/notebook

# FaceForensics++ Baseline (dlib & no internet)

https://www.kaggle.com/code/robikscube/faceforensics-baseline-dlib-no-internet

- Dlib install 필수. dlib의 역할?
    1. **얼굴 랜드마크 감지** : dlib는 얼굴 랜드마크(눈, 코, 입의 모서리와 같은 인간 얼굴의 특정 지점)를 감지하는 강력한 알고리즘을 제공합니다. 이를 통해 얼굴 특징의 정확한 현지화가 가능해지며, 이는 이미지에서 얼굴을 인식하고 정렬하는 데 중요합니다.
    2. **얼굴 감지** : dlib에는 이미지나 비디오 스트림에서 얼굴을 식별할 수 있는 얼굴 감지기가 포함되어 있습니다. 선형 분류기와 결합된 HOG(Histogram of Oriented Gradients) 방법을 사용하며 CNN 기반 얼굴 감지기와 같은 보다 고급 기술도 지원합니다.
    3. **얼굴 인코딩** : 얼굴이 감지되면 dlib는 해당 얼굴에 대한 128차원 얼굴 기술자를 계산할 수 있습니다. 이 기술자는 얼굴의 필수 특징을 포착하여 다양한 얼굴 간의 효율적인 비교를 가능하게 합니다. 조명, 포즈 및 표정의 변화에 대한 일정 수준의 허용 오차를 허용합니다.
    4. **다양한 모델 지원** : dlib는 얼굴 감지, 랜드마크 감지, 얼굴 인식을 포함한 다양한 작업을 위한 여러 가지 사전 학습된 모델을 지원하므로, 처음부터 모델을 학습시키지 않고도 확립된 방법으로 쉽게 작업을 시작할 수 있습니다.
    5. **머신 러닝 파이프라인과의 통합** : dlib는 더 큰 머신 러닝 워크플로에 쉽게 통합될 수 있습니다. 이를 통해 사용자는 dlib에서 생성된 인코딩을 사용하여 사용자 지정 인식 시스템을 구축하고 다양한 분류 방법(예: SVM 또는 클러스터링 알고리즘)을 적용할 수 있습니다.
    6. **실시간 처리** : dlib 알고리즘의 효율성 덕분에 실시간 얼굴 인식 기능이 가능해 보안 시스템, 사용자 인증, 대화형 사용자 인터페이스와 같은 애플리케이션에 적합합니다.

## **Pretrained Models from FaceForensics++**

- The authors provide different types of models:
    - From Appendix 4 of the paper we see the published accuracy:
        
        ![image.png]([https://prod-files-secure.s3.us-west-2.amazonaws.com/afca1fa2-51fe-446e-bfc2-70c740c52f50/6fcab5b4-5393-428d-94f1-20ee01cb0ac8/image.png](https://www.notion.so/SW_-11c273f9b9ae80da91c3f709909888c7?pvs=4#11d273f9b9ae80888d6fcaa5ce055e74))
        
    - We have:
        - Full image models
        - Face_detected models
        - Raw, compressed 23, and compressed 40
    
- Create a prediction function.
    - This function is provided a video file, and runs the test_full_image_network
    - Takes in the model file.
    - Save the output avi file.
    - Display some of the frames of the output video.

## Testing Full Image Models

- full_raw.p(full resolution) —> 원본 고해상도 모델
    - 압축되지 않은 원본 이미지로 학습된 모델, 이미지의 세부 픽셀 정보까지 모두 사용하여 학습
    - 장점: 고해상도의 이미지로 학습되기 때문에, 딥페이크가 적용된 얼굴의 미세한 차이점(텍스처 변화, 얼굴 윤곽)을 효과적으로 탐지할 수 있다.
    - 적용 사례: 영상이나 이미지의 원본 해상도를 유지하는 상황에서 매우 정확한 탐지가 필요할 때 유리하다.
    - 난이도: 높은 품질의 데이터를 사용하기 때문에, 계산량이 많고 학습에 많은 시간이 필요하지만 매우 정확한 결과를 기대할 수 있다. **난이도: 높음**.
    
- full_c40.p model
    - C40 압축을 거친 이미지로 학습된다. 더 높은 압축 비율을 적용한 상태로, 이미지 품질이 더 낮아진다.
    - 장점: 더 낮은 품질의 이미지를 학습하게 되어, 실제 환경에서 품질이 떨어진 영상 또는 이미지에서도 딥페이크를 탐지하는 능력을 향상시킨다.
    - 적용 사례: 인테넷 스트링밍 또는 소셜 미디어에서 흔히 볼 수 있는 낮은 품질의 이미지나 비디오에서 딥페이크를 탐지할 때 유용하다.
    - 난이도: 데이터 품질이 떨어지는 만큼, 노이즈를 다루는 능력이 요구되지만 학습 자체는 full_raw에 비해 상대적으로 쉽다. **난이도: 중간**.
    
- full_c23.p model
    - C23 압축을 사용한 이미지로 학습된 모델, C23 압축은 C40보다 낮은 압축 수준으로, 중간 정도의 품질을 유지한다.
    - 장점: C40과 raw 중간 정도에 해당하는 이미지 품질을 다룰 수 있어, 다양한 환경에서 적응력이 뛰어나다.
    - 적용사례: 적당한 압축이 가해진 이미지나 비디오, 예를 들어, 일부 스트리밍 환경이나 파일 공유 환경에서 딥페이크 탐지에 활용된다.
    - 난이도: 중간 품질의 데이터를 다루므로 full_raw와 full_c40 사이의 난이도로, 적응력이 필요한 환경에서 주로 사용된다. **난이도: 중간**.
    
- 이러한 모델들은 각각의 특성에 따라 데이터 전처리와 학습 전략을 달리해야 하므로, 목표로 하는 사용 환경에 맞는 모델을 선택하는 것이 중요합니다.

## Testing face detection/xception model

**1.  FaceForensics++에서의 Xception 모델**

- 얼굴 탐지에 특성화
- **Xception 모델 설명**: Xception(Extreme Inception)은 Inception 아키텍처에서 파생된 모델로, 더 깊고 효율적인 구조로 설계되었습니다. **심층 분리 합성곱(deep separable convolutions)**을 사용해 파라미터 수를 줄이면서도 학습 성능을 극대화하는 것이 특징입니다.
- **FaceForensics++에서의 역할**: 이 모델은 딥페이크 및 변형된 비디오에서 실제 얼굴과 가짜 얼굴을 구분하는 데 사용됩니다. 주로 **이진 분류**(진짜 vs 가짜) 문제를 해결하는 데 적합하며, 특히 텍스처 기반 변화를 감지하는 데 강력한 성능을 보입니다.
    - 요약
        - **Depthwise Separable Convolutions**: 공간적 차원과 깊이 차원에서 각각 별도의 컨볼루션을 수행하여 계산 효율성을 높이고 성능을 개선하는 핵심 기법입니다.
        - **Residual Connections**: 입력 정보의 일부를 그대로 전달하여 정보 손실을 줄이고 학습을 안정화합니다.

2. Testing face_detection/xception 모델

- 데이터 전처리: Xception 모델을 FaceForensics++에서 사용하려면 먼저 데이터를 처리해야 합니다. 일반적으로 다음과 같은 절차가 포함됩니다:
    - **얼굴 감지(face detection)**: 각 프레임에서 얼굴 영역을 감지해 **프레임을 크롭**하고, 탐지 성능을 높이기 위해 **크기 조정**(128x128 같은 표준 크기로 조정)합니다.
    - **정규화 및 텐서 변환**: 이미지를 모델 입력에 맞게 **정규화**한 후, 텐서로 변환해 PyTorch 또는 TensorFlow 같은 딥러닝 프레임워크에서 학습할 수 있게 준비합니다.
- 모델 학습 및 테스트:
    - Xception 모델을 **사전 훈련된 가중치**로 시작할 수 있으며, 이를 FaceForensics++ 데이터셋에 맞게 **파인튜닝**합니다. 학습 데이터는 **원본(C23, C40)**과 같이 압축된 데이터 또는 압축되지 않은 데이터를 포함합니다.
    - **테스트 단계**에서는 미리 처리된 테스트 비디오나 이미지에 대해 모델이 학습한 특징을 바탕으로 진짜와 가짜를 분류하게 됩니다. 각 프레임에서의 예측을 종합해 **비디오 레벨에서의 예측**도 가능합니다.

3. FaceForensics++ 데이터 전처리

- **얼굴 크롭(face cropping)**: 비디오에서 얼굴을 감지한 후, 얼굴 영역을 잘라냅니다. 이를 통해 모델이 필요하지 않은 배경 정보 대신 얼굴 특징에 집중하도록 한다.
- **데이터 증강**: 데이터 불균형을 해결하기 위해, 이미지의 회전, 크기 조정, 밝기 조정 등의 **데이터 증강** 기법을 적용할 수 있다.
- **프레임 샘플링**: 모든 프레임을 사용하기보다는 일정한 간격으로 **프레임을 샘플링**하여 학습에 사용합니다. 이를 통해 연산 자원을 줄이고 학습 속도를 높일 수 있다.

4. 난이도

- **Xception 모델 난이도**: FaceForensics++와 같은 고성능 데이터셋에서 Xception 모델을 사용하는 것은 매우 효과적이지만, **데이터 전처리**와 **모델 튜닝**이 상당히 중요하다. 따라서 초보자에게는 약간 복잡할 수 있으며, **중급에서 고급** 수준의 딥러닝 지식이 필요, 모델의 구조 이해, 파인튜닝, 데이터 증강 등의 작업이 어려운 부분에 속한다.
- Starter Kit과 비교
    - FaceForensics++에서 Xception 모델은 매우 강력한 성능을 제공하며, **starter kit**에서 제공하는 CNN 기반 모델보다 훨씬 깊고 복잡한 구조를 가지고 있습니다. 이는 특히 데이터의 미세한 변화를 잘 포착하는 데 적합하므로, **정밀한 딥페이크 탐지**를 목표로 할 때 더 나은 성능을 기대할 수 있다
1. 그외.
    1. XceptionNet은 FaceForensics++에서 광범위하게 사용되는 모델 중 하나
    2. EfficientNet:
        - 네트워크의 크기(깊이, 너비, 해상도)를 균형적으로 확장하는 방법론을 사용합니다. 이는 더 적은 파라미터와 더 높은 성능을 제공하며, FaceForensics++에서 다양한 비디오 조작을 탐지하는 데 효과적입니다.
        - **특징**: 네트워크 구조를 효율적으로 확장하여 더 적은 연산 자원으로도 높은 성능을 낼 수 있습니다
    3. ResNet:
        - **설명**: Residual Networks(ResNet)는 매우 깊은 신경망을 학습할 수 있는 구조로, 깊은 네트워크에서 발생하는 기울기 소실 문제를 해결하기 위해 skip connections(잔차 연결)을 사용합니다. FaceForensics++에서의 ResNet 모델은 주로 얼굴 변조의 미세한 패턴을 탐지하는 데 사용됩니다.
        - **특징**: 안정적인 학습과 깊은 네트워크의 장점을 통해 높은 변조 탐지 성능을 발휘합니다.
    4. **VGG16/VGG19:**
        - **설명**: VGGNet은 매우 깊은 CNN 구조로, 16개 또는 19개의 층을 통해 특징을 추출합니다. FaceForensics++에서는 이러한 깊은 네트워크를 통해 고해상도 이미지나 비디오에서 변조를 탐지하는 데 사용됩니다.
        - **특징**: 대규모 특징 추출을 통해 정교한 변조 감지가 가능합니다.
    5. **LSTM 기반 모델:**
        - **설명**: FaceForensics++에서는 시간적 의존성을 고려하여 LSTM(Long Short-Term Memory) 모델을 사용하기도 합니다. 이 모델은 비디오의 연속 프레임 간의 관계를 분석하여 변조 패턴을 감지합니다.
        - 비디오 클립의 프레임 간 상관관계를 분석하여 연속적인 변조 패턴을 발견하는 데 효과적입니다.
2. 다양한 모델 결합 학습 —> 앙상블
    1. **Stacking 앙상블:**
        - 여러 모델의 출력을 결합해 새로운 메타 모델(meta model)이 학습하도록 하는 기법입니다. 예를 들어, ResNet과 XceptionNet, EfficientNet 등의 모델을 학습시킨 후, 이들의 출력값을 입력으로 받아 최종 예측을 하는 모델을 추가로 학습시킵니다. 메타 모델로는 간단한 회귀 모델이나 작은 신경망을 사용할 수 있습니다.
    2. **Voting 앙상블:**
        - 여러 모델의 예측 결과를 투표 방식으로 결합합니다. 각 모델의 예측이 하나의 클래스(예: 딥페이크 여부)에 대해 출력되며, 다수의 모델이 예측한 결과를 최종 예측값으로 선택하는 방식입니다. 이 방법은 다양한 딥페이크 탐지에서 많이 사용되며, 각 모델의 성능을 고려한 가중 투표(weighted voting)를 사용할 수도 있습니다.
    3. **Bagging:**
        - Bagging은 여러 모델을 병렬적으로 학습시켜 각각의 예측을 결합하는 방법입니다. 다양한 CNN 모델들을 독립적으로 학습시킨 후 그 결과를 결합하여 최종 결과를 도출할 수 있습니다.
    4. **Blending:**
        - Stacking과 유사하지만, 훈련 데이터의 일부만을 사용해 모델을 학습시키고 나머지 데이터로 메타 모델을 학습시킵니다. 이 방법은 데이터가 많지 않은 경우에 효과적일 수 있습니다.
    - FaceForensics++와 같은 데이터셋에서 ResNet, EfficientNet, Xception 같은 모델을 앙상블 학습 방식으로 결합하면 각 모델이 딥페이크 탐지에서 잘하는 부분들을 통합하여 더 높은 정확도를 얻을 수 있습니다.

### 학습 방향성

1. 방향:
    - **풀 이미지(Full Images)**: 이 방법은 전체 프레임에서 조작된 부분을 탐지하는 데 초점을 맞춥니다. 이 경우, 이미지의 모든 부분이 모델에 입력으로 들어가고, 전체 프레임에서 이상이 있는 부분을 찾습니다. 풀 이미지 학습은 조작이 얼굴뿐 아니라 배경이나 기타 요소에서 이루어질 가능성이 있을 때 유리합니다. 이 방식은 처리할 데이터의 크기가 크고 계산 자원이 더 많이 필요하지만, 전체적인 변조 감지를 포함할 수 있는 장점이 있습니다.
    - **얼굴 부분만 자른 이미지(Cropped Faces)**: 얼굴에만 집중하는 방식으로, 이는 딥페이크와 같이 얼굴 변조에만 초점을 맞춘 탐지 방법에 적합합니다. FaceForensics++에서는 대개 얼굴 탐지 알고리즘(MTCNN, Dlib 등)을 사용하여 비디오 프레임에서 얼굴 부분을 잘라내고 이를 학습 데이터로 사용합니다. 이 방법은 얼굴 변조가 주요 이슈일 때 유리하며, 데이터 크기가 줄어들어 모델 학습이 더 효율적일 수 있습니다. 다만, 얼굴 외의 변조를 탐지하기 어렵습니다.
- 전략:
    - **병렬 학습**: FaceForensics++는 풀 이미지와 얼굴 부분만 자른 이미지를 동시에 학습시킬 수 있습니다. 두 가지 데이터를 모두 사용하면 딥페이크 탐지 성능을 높일 수 있습니다. 얼굴 변조뿐 아니라, 얼굴 외의 변조도 탐지하는 데 유용합니다.
    - **데이터 전처리 차이**: 얼굴 부분을 잘라내는 경우, 보통은 얼굴 탐지기가 프레임에서 얼굴 영역을 추출한 뒤, 이미지 해상도를 정규화하고, 그 영역만을 CNN 모델에 입력합니다. 풀 이미지를 학습할 때는 전체 프레임을 대상으로 같은 절차를 수행하지만, 데이터 크기가 더 큽니다.
