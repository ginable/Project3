# 오픈소스 중간발표 준비

# 1. 데이터 전처리

### 패키지 설치

```python
!pip install facenet-pytorch
!pip install --upgrade torch torchvision
```

### 패키지 import

```python
import os
import zipfile
import cv2
import torch
from facenet_pytorch import MTCNN
from PIL import Image
from tqdm import tqdm
from google.colab import drive
from google.colab import drive
drive.mount('/content/drive')
```

### 파일정리

```python
video_folder = '/content/drive/MyDrive/datasets/Video_maindata'  # 비디오가 저장될 폴더
output_folder = '/content/drive/MyDrive/datasets/Image_maindata'  # 얼굴 이미지 저장 폴더

# 압축 파일 리스트
zip_file_paths = [
    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_00.zip',
    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_01.zip',
    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_02.zip',
    '/content/drive/MyDrive/datasets/train_sample_videos.zip'
]

# 각 ZIP 파일 압축 해제
for zip_file_path in zip_file_paths:
    print(f"Extracting {zip_file_path}...")
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(video_folder)
    print(f"Finished extracting {zip_file_path}")

print("모든 ZIP 파일이 성공적으로 압축 해제되었습니다.")
```

### GPU CPU 설정

```python
# Step 4: GPU 또는 CPU 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"사용 중인 장치: {device}")

# Step 5: GPU를 사용한 MTCNN 초기화 ( 각 프레임에서 얼굴을 검출 )
mtcnn = MTCNN(keep_all=True, device=device) 
```

```python
# Step 6: 추가 설정
frame_interval = 10  # 프레임 간격
resize_dim = (224, 224)  # 얼굴 이미지 크기
```

### 비디오 처리 함수

```python
# Step 7: 비디오 처리 함수
def process_video(video_path, output_folder, mtcnn, frame_interval, resize_dim):
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    extracted_count = 0
    video_name = os.path.splitext(os.path.basename(video_path))[0]

    # 비디오별 하위 폴더 생성
    video_output_folder = os.path.join(output_folder, video_name)
    os.makedirs(video_output_folder, exist_ok=True)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # 프레임 간격에 따라 샘플링
        if frame_count % frame_interval == 0: # 매 10번째 프레임 마다 얼굴 검출이 수행
            # OpenCV BGR 이미지를 PIL RGB로 변환
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame_rgb)

            # MTCNN을 통한 얼굴 감지
            boxes, _ = mtcnn.detect(pil_img)
            if boxes is not None:
                for i, box in enumerate(boxes):
                    # 얼굴 크롭 및 리사이즈 (x1, y1, x2, y2 형식), 224 x 224 로 resize
                    face = pil_img.crop((box[0], box[1], box[2], box[3])).resize(resize_dim)
                    face.save(os.path.join(video_output_folder, f"{video_name}_face_{extracted_count}.jpg"))
                    extracted_count += 1

        frame_count += 1

    cap.release()
    
    # Step 8: 하위 폴더의 모든 비디오 파일을 가져오기
video_files = []
for root, dirs, files in os.walk(video_folder):
    for file in files:
        if file.endswith('.mp4'):
            video_files.append(os.path.join(root, file))

if not video_files:
    print("No .mp4 files found in the extracted folders. Check if the ZIP files contain .mp4 videos.")
else:
    print(f"총 {len(video_files)}개의 비디오 파일이 발견되었습니다.")
    
    # Step 9: 모든 비디오 파일 처리
for folder in os.listdir(video_folder):
    folder_path = os.path.join(video_folder, folder)
    if os.path.isdir(folder_path):
        video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]
        for video_file in tqdm(video_files, desc=f"{folder} 비디오 처리 중"):
            video_path = os.path.join(folder_path, video_file)
            process_video(video_path, output_folder, mtcnn, frame_interval, resize_dim)
            print(f"{video_file} 처리 완료")
```

![스크린샷 2024-11-10 오후 10.53.14.png](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-11-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_10.53.14.png)

## Model Train 01

### 패키지 설치

```python
# EfficientNet-PyTorch 설치
!pip install efficientnet-pytorch

# pretrainedmodels 설치 (Xception 모델용)
!pip install pretrainedmodels
```

### 패키지 import

```python
import os
import ssl
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, models
from torchvision.models import ResNet50_Weights
from efficientnet_pytorch import EfficientNet
from pretrainedmodels import xception
from PIL import Image
from tqdm import tqdm
from google.colab import drive
from torchvision import datasets, transforms
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
import random
```

### SSL 인증서 검증 비활성화

```python
# SSL 인증서 검증 비활성화 (Xceeption_Model을 문제 없이 다운로드 하기 위해)
ssl._create_default_https_context = ssl._create_unverified_context
```

### 학습데이터 구성

```python
class FaceForensicsDataset(Dataset):
    def __init__(self, image_folder, transform=None, sample_ratio=0.1):
        self.image_folder = image_folder # 데이터셋 구성
        self.transform = transform # 이미지 전처리 변환 객체 (데이터 증강 및 정규화 포함)
        self.images = []
        self.labels = []

        # 우선적으로 sample_train_data 디렉토리 내 모든 파일 추가
        for root, dirs, files in os.walk(image_folder):
            if 'sample_train_data' in root:
                for img in files:
                    if img.endswith(('.jpg', '.png')):
                        img_path = os.path.join(root, img)
                        self.images.append(img_path)
                        self.labels.append(1 if "REAL" in img else 0)

        # 그 외 디렉토리에서 무작위 10% 샘플 선택
        for root, dirs, files in os.walk(image_folder):
            if 'sample_train_data' not in root:
                files = [f for f in files if f.endswith(('.jpg', '.png'))]
                sample_count = int(len(files) * sample_ratio)
                sampled_files = random.sample(files, sample_count)
                for img in sampled_files:
                    img_path = os.path.join(root, img)
                    self.images.append(img_path)
                    self.labels.append(1 if "REAL" in img else 0)

        print(f"Loaded {len(self.images)} images: all from 'sample_train_data' and 10% from other directories.")

    def __len__(self):
        return len(self.images) 

    def __getitem__(self, idx): # 최종적으로 이미지와 레이블 반환 - 모델 학습에 사용
        img_path = self.images[idx]
        label = self.labels[idx]
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, label

# 데이터 전처리 설정
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

# 데이터셋 로드
image_folder = '/content/drive/MyDrive/datasets/Image_maindata'
dataset = FaceForensicsDataset(image_folder=image_folder, transform=transform, sample_ratio=0.1)

# 학습 및 검증 데이터 분할 (8:2 비율)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# 데이터 로더 생성
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

print(f"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}")
```

### 학습 장치 설정

```python
# GPU 장치 설정 (Colab에서는 'cuda' 사용)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 모델 설정
# num_classes = 1000 -> 사전 학습된 ImageNet의 클래스 수
xception_model = xception(num_classes=1000, pretrained='imagenet')
# 마지막 완전 연결층 fc레이어 - 2진 분류를 출력 설정위해 nn.linear로 교체
xception_model.last_linear = nn.Linear(xception_model.last_linear.in_features, 2)
xception_model = xception_model.to(device)

efficientnet_model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2)
efficientnet_model = efficientnet_model.to(device)

resnet_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 2)
resnet_model = resnet_model.to(device)  # 모델을 cuda에 올리기
```

### 손실 함수 및 최적화 함수 설정 - loss = NaN  피하기 위해서

```python
# 손실 함수 및 최적화 함수 설정
criterion = nn.CrossEntropyLoss()
optimizer = Adam(
    list(xception_model.parameters()) +
    list(efficientnet_model.parameters()) +
    list(resnet_model.parameters()),
    lr=1e-5  # (1e-5)Adam 옵티마이저는 모든 모델의 파라미터를 최적화
)

# NaN 검사 함수
def check_nan(tensor, name=""):
    if torch.isnan(tensor).any():
        print(f"NaN detected in {name}")
        return True
    return False

# 앙상블 예측 함수 - 각 모델이 동일한 입력 이미지 배치에 대해 예측을 수행하고,평균으로 최종 예측값을 생성
def ensemble_predict(models, images):
    outputs = [model(images) for model in models]
    outputs = torch.stack(outputs).mean(dim=0)
    return outputs
```

### 학습 및 검증 함수

```python
# 학습 및 검증 함수 정의
def train_ensemble_model(models, criterion, optimizer, num_epochs=5):
    for epoch in range(num_epochs):
        for model in models:
            model.train()

        train_loss, train_correct = 0.0, 0
        for images, labels in tqdm(train_loader, desc=f"Training Epoch {epoch+1}/{num_epochs}"):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = ensemble_predict(models, images)

            # NaN 검사
            if check_nan(outputs, "outputs"):
                return

            loss = criterion(outputs, labels)
            if check_nan(loss, "loss"):
                return
						
						# 역전파 및 최적화 수행
            loss.backward() 
            optimizer.step()

            train_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            train_correct += (preds == labels).sum().item()

				# Epoch 종료 시 평균 손실과 정확도 출력
        train_acc = train_correct / len(train_dataset)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}")

        # Validation
        for model in models:
            model.eval()

        val_loss, val_correct = 0.0, 0
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc=f"Validation Epoch {epoch+1}/{num_epochs}"):
                images, labels = images.to(device), labels.to(device)
                outputs = ensemble_predict(models, images)
                if check_nan(outputs, "validation outputs"):
                    return

                loss = criterion(outputs, labels)
                if check_nan(loss, "validation loss"):
                    return

                val_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                val_correct += (preds == labels).sum().item()

        val_acc = val_correct / len(val_dataset)
        print(f"Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_acc:.4f}")
```

### 학습 시작

```python
# 학습 시작
models = [xception_model, efficientnet_model, resnet_model]
train_ensemble_model(models, criterion, optimizer, num_epochs=5)
```

![스크린샷 2024-11-10 오후 10.53.36.png](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-11-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_10.53.36.png)

### 모델 저장

```python
# 모델 저장 경로 설정
output_dir = '/content/drive/MyDrive/datasets'
os.makedirs(output_dir, exist_ok=True)

# 모델 저장
torch.save(xception_model.state_dict(), os.path.join(output_dir, 'xception_model_02.pth'))
torch.save(efficientnet_model.state_dict(), os.path.join(output_dir, 'efficientnet_model_02.pth'))
torch.save(resnet_model.state_dict(), os.path.join(output_dir, 'resnet_model_02.pth'))

# 모델이 성공적으로 저장되었는지 확인
if os.path.exists(xception_path) and os.path.exists(efficientnet_path) and os.path.exists(resnet_path):
    print("모델이 성공적으로 저장되었습니다.")
else:
    print("모델 저장에 실패했습니다.")
```

## Model Train 02 ( sampling 10% )

### 추가된 코드만 작성

```python
# Step 7: 비디오 처리 함수
def process_video(video_path, output_folder, mtcnn, frame_interval, resize_dim):
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    extracted_count = 0
    video_name = os.path.splitext(os.path.basename(video_path))[0]

    # 비디오별 하위 폴더 생성
    video_output_folder = os.path.join(output_folder, video_name)
    os.makedirs(video_output_folder, exist_ok=True)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # 프레임 간격에 따라 샘플링
        if frame_count % frame_interval == 0:
            # OpenCV BGR 이미지를 PIL RGB로 변환
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame_rgb)

            # MTCNN을 통한 얼굴 감지
            boxes, _ = mtcnn.detect(pil_img)
            if boxes is not None:
                for i, box in enumerate(boxes):
                    # 얼굴 크롭 및 리사이즈
                    face = pil_img.crop((box[0], box[1], box[2], box[3])).resize(resize_dim)
                    face.save(os.path.join(video_output_folder, f"{video_name}_face_{extracted_count}.jpg"))
                    extracted_count += 1

        frame_count += 1

    cap.release()
```

### 학습 시작

```python
# 학습 시작
models = [xception_model, efficientnet_model, resnet_model]
train_ensemble_model(models, criterion, optimizer, num_epochs=5)
```

![스크린샷 2024-11-10 오후 10.03.07.png](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-11-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_10.03.07.png)

### 모델 저장

```python
# 모델 저장 경로 설정
output_dir = '/content/drive/MyDrive/datasets'
os.makedirs(output_dir, exist_ok=True)

# 모델 저장
torch.save(xception_model.state_dict(), os.path.join(output_dir, 'xception_model_02.pth'))
torch.save(efficientnet_model.state_dict(), os.path.join(output_dir, 'efficientnet_model_02.pth'))
torch.save(resnet_model.state_dict(), os.path.join(output_dir, 'resnet_model_02.pth'))

print("모델이 성공적으로 저장되었습니다.")
```

## Model Evaluation 01 (test_videos - 400개의 fake_videos)

### 패키지 import

```python
import os
import torch
import timm
from torchvision import models, transforms
from PIL import Image
import cv2
```

### 모델 불러오기

```python
# GPU 사용 가능 여부 확인
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"사용 중인 장치: {device}")

# 모델 불러오기 (timm을 사용하여 Xception, EfficientNet 모델 불러오기)
output_dir = '/content/drive/MyDrive/datasets'
models_dict = {
    'Xception': timm.create_model('xception', pretrained=False, num_classes=2).to(device),
    'EfficientNet': timm.create_model('efficientnet_b0', pretrained=False, num_classes=2).to(device),
    'ResNet': models.resnet50(num_classes=2).to(device)
}
```

```python
# 모델 가중치 로드
xception_state_dict = torch.load(os.path.join(output_dir, 'xception_model_02.pth'))
xception_state_dict = {k.replace("last_linear", "fc") if "last_linear" in k else k: v for k, v in xception_state_dict.items()}
models_dict['Xception'].load_state_dict(xception_state_dict, strict=False)
models_dict['EfficientNet'].load_state_dict(torch.load(os.path.join(output_dir, 'efficientnet_model_02.pth')), strict=False)
models_dict['ResNet'].load_state_dict(torch.load(os.path.join(output_dir, 'resnet_model_02.pth')), strict=False)

# 모델을 평가 모드로 설정
for model in models_dict.values():
    model.eval()

# 이미지 전처리 파이프라인 정의
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```

### 함수 선언

```python
# 비디오를 읽어 REAL 또는 FAKE 예측 수행
def predict_video(video_path, models_dict, preprocess, sample_rate=10):
    cap = cv2.VideoCapture(video_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_predictions = {model_name: [] for model_name in models_dict.keys()}

    for i in range(frame_count):
        ret, frame = cap.read()
        if not ret:
            break
        # 매 sample_rate 프레임마다 예측 수행
        if i % sample_rate == 0:
            # OpenCV 이미지를 PIL 이미지로 변환
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            input_tensor = preprocess(pil_image).unsqueeze(0).to(device)

            # 각 모델로 예측 수행
            for model_name, model in models_dict.items():
                with torch.no_grad():
                    output = model(input_tensor)
                    _, predicted = torch.max(output, 1)
                    frame_predictions[model_name].append(predicted.item())

    cap.release()

    # 각 모델의 프레임 예측 결과를 종합하여 비디오 예측 결정
    video_predictions = {}
    for model_name, predictions in frame_predictions.items():
        real_count = predictions.count(1)
        fake_count = predictions.count(0)
        video_predictions[model_name] = 'REAL' if real_count > fake_count else 'FAKE'

    return video_predictions

```

### 예측 수행

```python
# 비디오를 읽어 REAL 또는 FAKE 예측 수행
def predict_video(video_path, models_dict, preprocess, sample_rate=10):
    cap = cv2.VideoCapture(video_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_predictions = {model_name: [] for model_name in models_dict.keys()}

    for i in range(frame_count):
        ret, frame = cap.read()
        if not ret:
            break
        # 매 sample_rate 프레임마다 예측 수행
        if i % sample_rate == 0:
            # OpenCV 이미지를 PIL 이미지로 변환
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            input_tensor = preprocess(pil_image).unsqueeze(0).to(device)

            # 각 모델로 예측 수행
            for model_name, model in models_dict.items():
                with torch.no_grad():
                    output = model(input_tensor)
                    _, predicted = torch.max(output, 1)
                    frame_predictions[model_name].append(predicted.item())

    cap.release()

    # 각 모델의 프레임 예측 결과를 종합하여 비디오 예측 결정
    video_predictions = {}
    for model_name, predictions in frame_predictions.items():
        real_count = predictions.count(1)
        fake_count = predictions.count(0)
        video_predictions[model_name] = 'REAL' if real_count > fake_count else 'FAKE'

    return video_predictions

```

![스크린샷 2024-11-10 오후 10.06.33.png](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-11-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_10.06.33.png)

[predictions.csv](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/predictions.csv)

### 한계

[fglewmddcn.mp4](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/fglewmddcn.mp4)

[fanibwbmoq.mp4](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/fanibwbmoq.mp4)

[metadata.json](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/metadata.json)

```python
# 비디오가 저장된 디렉터리 설정
video_dir = '/content/drive/MyDrive/datasets/00~02_video/aa'  # 실제 비디오 폴더 경로로 수정
video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]

# 결과를 저장할 리스트 초기화
results = []

# 각 비디오에 대해 예측 수행
for video_file in video_files:
    video_path = os.path.join(video_dir, video_file)
    predictions = predict_video(video_path, models_dict, preprocess)
    print(f"비디오: {video_file}")
        # 예측 결과를 저장 (비디오 이름 및 각 모델의 예측 결과)
    for model_name, prediction in predictions.items():
        results.append({'filename': video_file, 'model': model_name, 'label': prediction})
    for model_name, prediction in predictions.items():
        print(f"{model_name} 예측 결과: {prediction}")
    print("-" * 30)
```

- Real_Videos 2개를 평가해 본 결과

![스크린샷 2024-11-10 오후 10.10.59.png](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-11-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_10.10.59.png)

### 위의 모델 평가에 대하여

1. 데이터셋의 균형 조정: real과 fake 비율을 최대한 비슷하게 유지하여 학습

→ real 데이터 증강 또는 real과 fake 레이블에 대한 가중치 다르게 하기

1. 과적합 방지 기법 적용

→ 드롭아웃 레이어를 추가하여 일부 뉴런을 무작위로 학습에서 제외하여 모델이 특정 패턴에 의존하지 않도록 한다.

→ 정규화 기법(L2 정규화)를 적용해서 가중치 값이 너무 커지지 않도록 제어한다.

1. 하이퍼파라미터 조정

## 데이터 증강 - 좌우 반전

### 증강 예시

![스크린샷 2024-11-10 오후 9.51.07.png](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-11-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_9.51.07.png)

### 패키지 import

```python
import os
import json
from PIL import Image, ImageOps
import torch
from torchvision import transforms
```

### 경로 설정 및 이미지 변환 설정

```python
# 경로 설정
image_folder = '/content/drive/MyDrive/datasets/sample_train_data'  # 비디오에서 추출된 이미지가 저장된 폴더
json_path = '/content/drive/MyDrive/datasets/train_sample_metadata.json'  # JSON 라벨 파일 경로
augmented_image_folder = '/content/drive/MyDrive/datasets/sample_train_data/aug_trainsample'  # 증강 이미지 저장 폴더
os.makedirs(augmented_image_folder, exist_ok=True)

# 이미지 변환 설정 (텐서 변환 포함)
transform_to_tensor = transforms.Compose([
    transforms.Resize((224, 224)),  # 원하는 크기로 리사이즈
    transforms.ToTensor(),  # 텐서로 변환
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 정규화
    
# JSON 파일에서 라벨 데이터 불러오기
with open(json_path, 'r') as f:
    labels = json.load(f)  # { "비디오파일명": {"label": "REAL" 또는 "FAKE"}, ... } 형태
```

### 이미지 증강

```python
# 'REAL' 레이블에 해당하는 이미지 증강 및 텐서 변환
total_videos = sum(1 for v in labels.values() if v["label"] == "REAL")
current_video_count = 0

for video_filename, video_info in labels.items():
    if video_info["label"] == "REAL":
        current_video_count += 1
        print(f"[{current_video_count}/{total_videos}] Processing video '{video_filename}' (REAL label)")

        # 비디오 파일명으로 시작하는 이미지 파일들을 찾기 위한 패턴 설정
        video_prefix = os.path.splitext(video_filename)[0]  # 확장자 제거 (예: eudeqjhdfd)
        images_in_video = [
            os.path.join(image_folder, img) 
            for img in os.listdir(image_folder) 
            if img.startswith(f"{video_prefix}_face") and img.endswith(('.jpg', '.png'))
        ]

        # 이미지가 발견되지 않는 경우 경고 메시지 출력
        if not images_in_video:
            print(f"  - Warning: No images found for video '{video_filename}' in folder '{image_folder}'.")
            continue
        else:
            print(f"  - Found {len(images_in_video)} images for video '{video_filename}'.")

        # 이미지 증강 및 저장
        for img_path in images_in_video:
            try:
                # 이미지 열기
                image = Image.open(img_path).convert("RGB")
                print(f"    - Loaded image '{img_path}' successfully.")

                # 좌우 반전 이미지 생성
                flipped_image = ImageOps.mirror(image)
                
                # 증강된 이미지를 텐서로 변환
                tensor_image = transform_to_tensor(flipped_image)
                
                # 반전된 이미지 저장 (파일명에 "_flipped" 추가)
                img_filename = os.path.basename(img_path)
                flipped_filename = f"{os.path.splitext(img_filename)[0]}_flipped{os.path.splitext(img_filename)[1]}"
                flipped_image_save_path = os.path.join(augmented_image_folder, flipped_filename)
                
                # 저장 및 예외 처리
                try:
                    flipped_image.save(flipped_image_save_path)
                    print(f"    - Saved augmented image '{flipped_filename}' at '{flipped_image_save_path}'.")
                except Exception as e:
                    print(f"    - Failed to save augmented image '{flipped_filename}'. Error: {e}")
                
                # 출력: 이미지 텐서 정보
                print(f"    - Processed tensor for {flipped_filename}, tensor shape: {tensor_image.shape}")

            except Exception as e:
                print(f"    - Failed to load or process image '{img_path}' for video '{video_filename}'. Error: {e}")

print("모든 REAL 비디오에 대한 증강 작업 및 텐서 변환이 완료되었습니다.")
```

[real_videos_labels.json](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/real_videos_labels.json)

# <Appendix>

### FaceForensics++의 장점

1. 다양한 모델 성능 평가: XceptionNet, EfficientNet, ResNet과 같은 검증된 모델들을 활용하여 데이터셋의 성능 높일 수 있다. DeepFake Detection에 좋은 성능을 보이고 모델의 정확도와 안정성을 높일 수 있다.
2. 얼굴 검출 및 크롭, 데이터 증강 등의 전처리 과정을 효율적으로 구현하는 방법을 제공.
    - XceptionNet: 심층 분리 합성곱을 통해 연산 효율성을 극대화, 얼굴 영역의 텍스처 변화를 잘 감지한다.
    - EfficientNet: 네트워크의 크기를 균형적으로 확장하는 방식, 크기, 깊이, 너비 등을 확장하면서도 연산 효율성을 유지한다.
    - ResNet: 미세한 조작 패턴을 안정적으로 학습, 딥페이크의 섬세한 변화를 잘 탐지 한다.
    

![스크린샷 2024-11-09 오후 8.44.53.png](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-11-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_8.44.53.png)

# <추가적으로 시도해 볼 것들>

1. ReduceROnPlateau 스케줄러 사용하기 - 검증 손실을 기준으로 학습률을 동적으로 조절하는 코드
2. 학습 데이터 추가 (00.~09. 데이터 추가 및 realvideo 증강)
3. 앙상블 모델 여러가지 시도