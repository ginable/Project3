# 오픈소스 중간발표 준비

# 1. 데이터 전처리

### 패키지 설치

```python
!pip install facenet-pytorch
!pip install --upgrade torch torchvision
```

### 패키지 import

```python
import os
import zipfile
import cv2
import torch
from facenet_pytorch import MTCNN
from PIL import Image
from tqdm import tqdm
from google.colab import drive
from google.colab import drive
drive.mount('/content/drive')
```

### 파일정리

```python
video_folder = '/content/drive/MyDrive/datasets/Video_maindata'  # 비디오가 저장될 폴더
output_folder = '/content/drive/MyDrive/datasets/Image_maindata'  # 얼굴 이미지 저장 폴더

# 압축 파일 리스트
zip_file_paths = [
    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_00.zip',
    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_01.zip',
    '/content/drive/MyDrive/datasets/Main_DATA/dfdc_train_part_02.zip',
    '/content/drive/MyDrive/datasets/train_sample_videos.zip'
]

# 각 ZIP 파일 압축 해제
for zip_file_path in zip_file_paths:
    print(f"Extracting {zip_file_path}...")
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(video_folder)
    print(f"Finished extracting {zip_file_path}")

print("모든 ZIP 파일이 성공적으로 압축 해제되었습니다.")
```

### GPU CPU 설정

```python
# Step 4: GPU 또는 CPU 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"사용 중인 장치: {device}")

# Step 5: GPU를 사용한 MTCNN 초기화 ( 각 프레임에서 얼굴을 검출 )
mtcnn = MTCNN(keep_all=True, device=device) 
```

```python
# Step 6: 추가 설정
frame_interval = 10  # 프레임 간격
resize_dim = (224, 224)  # 얼굴 이미지 크기
```

### 비디오 처리 함수

```python
# Step 7: 비디오 처리 함수
def process_video(video_path, output_folder, mtcnn, frame_interval, resize_dim):
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    extracted_count = 0
    video_name = os.path.splitext(os.path.basename(video_path))[0]

    # 비디오별 하위 폴더 생성
    video_output_folder = os.path.join(output_folder, video_name)
    os.makedirs(video_output_folder, exist_ok=True)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # 프레임 간격에 따라 샘플링
        if frame_count % frame_interval == 0: # 매 10번째 프레임 마다 얼굴 검출이 수행
            # OpenCV BGR 이미지를 PIL RGB로 변환
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame_rgb)

            # MTCNN을 통한 얼굴 감지
            boxes, _ = mtcnn.detect(pil_img)
            if boxes is not None:
                for i, box in enumerate(boxes):
                    # 얼굴 크롭 및 리사이즈 (x1, y1, x2, y2 형식), 224 x 224 로 resize
                    face = pil_img.crop((box[0], box[1], box[2], box[3])).resize(resize_dim)
                    face.save(os.path.join(video_output_folder, f"{video_name}_face_{extracted_count}.jpg"))
                    extracted_count += 1

        frame_count += 1

    cap.release()
    
    # Step 8: 하위 폴더의 모든 비디오 파일을 가져오기
video_files = []
for root, dirs, files in os.walk(video_folder):
    for file in files:
        if file.endswith('.mp4'):
            video_files.append(os.path.join(root, file))

if not video_files:
    print("No .mp4 files found in the extracted folders. Check if the ZIP files contain .mp4 videos.")
else:
    print(f"총 {len(video_files)}개의 비디오 파일이 발견되었습니다.")
    
    # Step 9: 모든 비디오 파일 처리
for folder in os.listdir(video_folder):
    folder_path = os.path.join(video_folder, folder)
    if os.path.isdir(folder_path):
        video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]
        for video_file in tqdm(video_files, desc=f"{folder} 비디오 처리 중"):
            video_path = os.path.join(folder_path, video_file)
            process_video(video_path, output_folder, mtcnn, frame_interval, resize_dim)
            print(f"{video_file} 처리 완료")
```

## Model Train

### 패키지 설치

```python
# EfficientNet-PyTorch 설치
!pip install efficientnet-pytorch

# pretrainedmodels 설치 (Xception 모델용)
!pip install pretrainedmodels
```

### 패키지 import

```python
import os
import ssl
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, models
from torchvision.models import ResNet50_Weights
from efficientnet_pytorch import EfficientNet
from pretrainedmodels import xception
from PIL import Image
from tqdm import tqdm
from google.colab import drive
from torchvision import datasets, transforms
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
import random
```

### SSL 인증서 검증 비활성화

```python
# SSL 인증서 검증 비활성화 (Xceeption_Model을 문제 없이 다운로드 하기 위해)
ssl._create_default_https_context = ssl._create_unverified_context
```

### 학습데이터 구성

```python
class FaceForensicsDataset(Dataset):
    def __init__(self, image_folder, transform=None, sample_ratio=0.1):
        self.image_folder = image_folder # 데이터셋 구성
        self.transform = transform # 이미지 전처리 변환 객체 (데이터 증강 및 정규화 포함)
        self.images = []
        self.labels = []

        # 우선적으로 sample_train_data 디렉토리 내 모든 파일 추가
        for root, dirs, files in os.walk(image_folder):
            if 'sample_train_data' in root:
                for img in files:
                    if img.endswith(('.jpg', '.png')):
                        img_path = os.path.join(root, img)
                        self.images.append(img_path)
                        self.labels.append(1 if "REAL" in img else 0)

        # 그 외 디렉토리에서 무작위 10% 샘플 선택
        for root, dirs, files in os.walk(image_folder):
            if 'sample_train_data' not in root:
                files = [f for f in files if f.endswith(('.jpg', '.png'))]
                sample_count = int(len(files) * sample_ratio)
                sampled_files = random.sample(files, sample_count)
                for img in sampled_files:
                    img_path = os.path.join(root, img)
                    self.images.append(img_path)
                    self.labels.append(1 if "REAL" in img else 0)

        print(f"Loaded {len(self.images)} images: all from 'sample_train_data' and 10% from other directories.")

    def __len__(self):
        return len(self.images) 

    def __getitem__(self, idx): # 최종적으로 이미지와 레이블 반환 - 모델 학습에 사용
        img_path = self.images[idx]
        label = self.labels[idx]
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, label

# 데이터 전처리 설정
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

# 데이터셋 로드
image_folder = '/content/drive/MyDrive/datasets/Image_maindata'
dataset = FaceForensicsDataset(image_folder=image_folder, transform=transform, sample_ratio=0.1)

# 학습 및 검증 데이터 분할 (8:2 비율)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# 데이터 로더 생성
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

print(f"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}")
```

### 학습 장치 설정

```python
# GPU 장치 설정 (Colab에서는 'cuda' 사용)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 모델 설정
# num_classes = 1000 -> 사전 학습된 ImageNet의 클래스 수
xception_model = xception(num_classes=1000, pretrained='imagenet')
# 마지막 완전 연결층 fc레이어 - 2진 분류를 출력 설정위해 nn.linear로 교체
xception_model.last_linear = nn.Linear(xception_model.last_linear.in_features, 2)
xception_model = xception_model.to(device)

efficientnet_model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2)
efficientnet_model = efficientnet_model.to(device)

resnet_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 2)
resnet_model = resnet_model.to(device)  # 모델을 cuda에 올리기
```

### 손실 함수 및 최적화 함수 설정 - loss = NaN  피하기 위해서

```python
# 손실 함수 및 최적화 함수 설정
criterion = nn.CrossEntropyLoss()
optimizer = Adam(
    list(xception_model.parameters()) +
    list(efficientnet_model.parameters()) +
    list(resnet_model.parameters()),
    lr=1e-5  # (1e-5)Adam 옵티마이저는 모든 모델의 파라미터를 최적화
)

# NaN 검사 함수
def check_nan(tensor, name=""):
    if torch.isnan(tensor).any():
        print(f"NaN detected in {name}")
        return True
    return False

# 앙상블 예측 함수 - 각 모델이 동일한 입력 이미지 배치에 대해 예측을 수행하고,평균으로 최종 예측값을 생성
def ensemble_predict(models, images):
    outputs = [model(images) for model in models]
    outputs = torch.stack(outputs).mean(dim=0)
    return outputs
```

### 학습 및 검증 함수

```python
# 학습 및 검증 함수 정의
def train_ensemble_model(models, criterion, optimizer, num_epochs=5):
    for epoch in range(num_epochs):
        for model in models:
            model.train()

        train_loss, train_correct = 0.0, 0
        for images, labels in tqdm(train_loader, desc=f"Training Epoch {epoch+1}/{num_epochs}"):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = ensemble_predict(models, images)

            # NaN 검사
            if check_nan(outputs, "outputs"):
                return

            loss = criterion(outputs, labels)
            if check_nan(loss, "loss"):
                return
						
						# 역전파 및 최적화 수행
            loss.backward() 
            optimizer.step()

            train_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            train_correct += (preds == labels).sum().item()

				# Epoch 종료 시 평균 손실과 정확도 출력
        train_acc = train_correct / len(train_dataset)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}")

        # Validation
        for model in models:
            model.eval()

        val_loss, val_correct = 0.0, 0
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc=f"Validation Epoch {epoch+1}/{num_epochs}"):
                images, labels = images.to(device), labels.to(device)
                outputs = ensemble_predict(models, images)
                if check_nan(outputs, "validation outputs"):
                    return

                loss = criterion(outputs, labels)
                if check_nan(loss, "validation loss"):
                    return

                val_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                val_correct += (preds == labels).sum().item()

        val_acc = val_correct / len(val_dataset)
        print(f"Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_acc:.4f}")
```

### 학습 시작

```python
# 학습 시작
models = [xception_model, efficientnet_model, resnet_model]
train_ensemble_model(models, criterion, optimizer, num_epochs=5)
```

### 모델 저장

```python
# 모델 저장 경로 설정
output_dir = '/content/drive/MyDrive/datasets'
os.makedirs(output_dir, exist_ok=True)

# 모델 저장
torch.save(xception_model.state_dict(), os.path.join(output_dir, 'xception_model_02.pth'))
torch.save(efficientnet_model.state_dict(), os.path.join(output_dir, 'efficientnet_model_02.pth'))
torch.save(resnet_model.state_dict(), os.path.join(output_dir, 'resnet_model_02.pth'))

# 모델이 성공적으로 저장되었는지 확인
if os.path.exists(xception_path) and os.path.exists(efficientnet_path) and os.path.exists(resnet_path):
    print("모델이 성공적으로 저장되었습니다.")
else:
    print("모델 저장에 실패했습니다.")
```

# <Appendix>

### FaceForensics++의 장점

1. 다양한 모델 성능 평가: XceptionNet, EfficientNet, ResNet과 같은 검증된 모델들을 활용하여 데이터셋의 성능 높일 수 있다. DeepFake Detection에 좋은 성능을 보이고 모델의 정확도와 안정성을 높일 수 있다.
2. 얼굴 검출 및 크롭, 데이터 증강 등의 전처리 과정을 효율적으로 구현하는 방법을 제공.
    - XceptionNet: 심층 분리 합성곱을 통해 연산 효율성을 극대화, 얼굴 영역의 텍스처 변화를 잘 감지한다.
    - EfficientNet: 네트워크의 크기를 균형적으로 확장하는 방식, 크기, 깊이, 너비 등을 확장하면서도 연산 효율성을 유지한다.
    - ResNet: 미세한 조작 패턴을 안정적으로 학습, 딥페이크의 섬세한 변화를 잘 탐지 한다.
    
< epoch-2 에서 accuracy = 0 loss = nan 이 나왔을때 손실 함수 및 최적화 함수 설정 이후 train_sample_data 적절한 학습의 결과>
![스크린샷 2024-11-09 오후 8.44.53.png](%E1%84%8B%E1%85%A9%E1%84%91%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A9%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%A1%E1%86%AB%E1%84%87%E1%85%A1%E1%86%AF%E1%84%91%E1%85%AD%20%E1%84%8C%E1%85%AE%E1%86%AB%E1%84%87%E1%85%B5%20139273f9b9ae8088a812d6dafa021448/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-11-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_8.44.53.png)

# <추가적으로 시도해 볼 것들>

1. ReduceROnPlateau 스케줄러 사용하기 - 검증 손실을 기준으로 학습률을 동적으로 조절하는 코드
2. 학습 데이터 추가
3. 앙상블 모델 여러가지 시도
